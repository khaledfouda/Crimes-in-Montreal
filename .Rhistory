mutate(PDQ = ifelse(! PDQ %in% spvm.pdq$PDQ, PDQ, -1)) %>%
table()
' Result: We have only two unknown PDQs: 11 and 24.
From searching spvm.qc.ca we found that they have equivalent PDQs.
'
data$PDQ[data$PDQ==24] = 26
data$PDQ[data$PDQ==11] = 9
#-------------------------------------------------------------------
#-------- 4. Merge the third dataset now that we matched the key column
data = left_join(data, spvm.pdq, c('PDQ')) #left_join doesn't sort the data
data %>%
select(CATEGORIE, DATE, QUART, PDQ,
DIVISION, LONGITUDE, LATITUDE,idnum) -> data
catg.fr = table(data$CATEGORIE) %>% names
catg.en = c("Resulting Death", "Break and Enter", "Mischief", "Auto Burglary",
"Auto theft", "Armed Robbery")
catg.rep = function(c) catg.en[which(catg.fr==c)]
quart.fr = table(data$QUART) %>% names
quart.en = c('Day', 'Night', 'Evening')
quart.rep = function(c) quart.en[which(quart.fr==c)]
# Apply changes and extract the Year and month
data %>% mutate(CATEGORIE = mapply(catg.rep,CATEGORIE),
DATE = as.Date(DATE, "%Y-%m-%d"),
QUART = mapply(quart.rep, QUART)) -> data
#-------------------------------------------------------------------
# --------- 7. Drop Some of the not-needed-anymore variables from the env.
rm(pdq, interv, spvm.pdq, catg.en, catg.fr, quart.en, quart.fr,
catg.rep, quart.rep)
#   -------- 8. Add two new columns for Month and Year
data$MONTH = month(data$DATE)
data$YEAR = year(data$DATE)
#-------------------------------------------------------------------
# -------- 8. Discard obsevations of the current month (June 2021)
data %>% filter( !(YEAR ==2021 & MONTH==6 )) -> data
#-----------------------------------------------------------------------
#------ 9. Save to disk or load from desk if it's been saved earlier
write.csv(data, './data/output/Police_Interventions_cleaned.csv')
input = read.csv('./data/output/Police_Interventions_cleaned.csv')
# The following function does the extraction of the full address given
# the coordinates.
get_addr = function(lat,long){
pull=reverse_geo(lat = lat, long = long, method = 'osm', verbose = FALSE)
addr = as.list(pull)$address
return(addr)
}
View(input)
#========================================
input = read.csv('./data/output/Police_Interventions_cleaned.csv',header=FALSE)
#========================================
input = read.csv('./data/output/Police_Interventions_cleaned.csv',header=T)
#========================================
input = read.csv('./data/output/Police_Interventions_cleaned.csv',header=F)
#========================================
input = read.csv('./data/output/Police_Interventions_cleaned.csv',row.names = F)
#========================================
input = read.csv('./data/output/Police_Interventions_cleaned.csv',row.names = NA)
#========================================
input = read.csv('./data/output/Police_Interventions_cleaned.csv',check.names = F)
#========================================
input = read.csv('./data/output/Police_Interventions_cleaned.csv',check.names = T)
#-----------------------------------------------------------------------
#------ 9. Save to disk or load from desk if it's been saved earlier
write.csv(data, './data/output/Police_Interventions_cleaned.csv',row.names = F)
#========================================
input = read.csv('./data/output/Police_Interventions_cleaned.csv')
out.query = data.frame(id=input$idnum, addrss=NA)
View(out.query)
#---
# subset the data so that we don't send NAs
input %>% filter(lat==NA)
#---
# subset the data so that we don't send NAs
input %>% filter(LATITUDE==NA)
#---
# subset the data so that we don't send NAs
input %>% filter(is.na(LATITUDE))
#---
# subset the data so that we don't send NAs
input %>% filter(is.na(LATITUDE)) -> input
#========================================
input = read.csv('./data/output/Police_Interventions_cleaned.csv')
#---
# subset the data so that we don't send NAs
input %>% filter(!is.na(LATITUDE)) -> input
# I will lookup 10^4 locations per request.
addrs.per.request = 10e4
# I will lookup 10^4 locations per request.
addrs.per.request = 1e4
maxn = nrow(input)
cat('Finished ',b)
b=5
cat('Finished ',b)
cat('Finished ',b, ' lookups out of ', maxn)
cat('Finished ',b, '/', maxn)
paste0('Finished ',b, '/', maxn)
cat('Finished ',b, '/', maxn)
cat('Finished ',b, '/', maxn, 'lookups.',maxn-b, 'are left.')
input = read.csv('./data/output/Police_Interventions_cleaned.csv')
# The following function does the extraction of the full address given
# the coordinates.
get_addr = function(lat,long){
#pull=reverse_geo(lat = lat, long = long, method = 'osm', verbose = FALSE)
#addr = as.list(pull)$address
addr = "eh?"
return(addr)
}
# We now apply the function to all the coordinates.
# Note that would take several hours to run.
# addrss: A list of all the full addresses.
out.query = data.frame(id=input$idnum, addrss=NA)
# subset the data so that we don't send NAs
input %>% filter(!is.na(LATITUDE)) -> input
# I will lookup 10^4 locations per request.
addrs.per.request = 1e4
maxn = nrow(input)
for(a in seq(1,maxn,addrs.per.request)){
b  = min(a+addrs.per.request-1,maxn)
out.query$addrss[input$idnum[a:b]] =
get_addr(input$LATITUDE[a:b],input$LONGITUDE[a:b])
# After each lookup (of 10^4 locations), the list is saved to disk.
print("writing to file")
write.csv(out.query,
'./data/processing/OSM_ADDS.csv', row.names = F)
cat('Finished ',b, '/', maxn, 'lookups.',maxn-b, 'are left.')
}
a
b
get_addr(input$LATITUDE[a:b],input$LONGITUDE[a:b])
out.query$addrss[input$idnum[a:b]]
b
b-a
out.query$addrss[input$idnum[a:b]] %>% length
input$LONGITUDE[a:b]
get_addr = function(lat,long){
#pull=reverse_geo(lat = lat, long = long, method = 'osm', verbose = FALSE)
#addr = as.list(pull)$address
addr = rep("eh?",length(lat))
return(addr)
}
# We now apply the function to all the coordinates.
# Note that would take several hours to run.
# addrss: A list of all the full addresses.
out.query = data.frame(id=input$idnum, addrss=NA)
# subset the data so that we don't send NAs
input %>% filter(!is.na(LATITUDE)) -> input
# I will lookup 10^4 locations per request.
addrs.per.request = 1e4
maxn = nrow(input)
for(a in seq(1,maxn,addrs.per.request)){
b  = min(a+addrs.per.request-1,maxn)
out.query$addrss[input$idnum[a:b]] =
get_addr(input$LATITUDE[a:b],input$LONGITUDE[a:b])
# After each lookup (of 10^4 locations), the list is saved to disk.
print("writing to file")
write.csv(out.query,
'./data/processing/OSM_ADDS.csv', row.names = F)
cat('Finished ',b, '/', maxn, 'lookups.',maxn-b, 'are left.')
}
get_addr(input$LATITUDE[a:b],input$LONGITUDE[a:b])
a
b
b-a
get_addr(input$LATITUDE[a:b],input$LONGITUDE[a:b]) %>% length()
input = read.csv('./data/output/Police_Interventions_cleaned.csv')
# The following function does the extraction of the full address given
# the coordinates.
get_addr = function(lat,long){
#pull=reverse_geo(lat = lat, long = long, method = 'osm', verbose = FALSE)
#addr = as.list(pull)$address
addr = rep("eh?",length(lat))
return(addr)
}
# We now apply the function to all the coordinates.
# Note that would take several hours to run.
# addrss: A list of all the full addresses.
out.query = data.frame(id=input$idnum, addrss=NA)
# subset the data so that we don't send NAs
input %>% filter(!is.na(LATITUDE)) -> input
# I will lookup 10^4 locations per request.
addrs.per.request = 1e4
maxn = nrow(input)
for(a in seq(1,maxn,addrs.per.request)){
b  = min(a+addrs.per.request-1,maxn)
out.query$addrss[input$idnum[a:b]] =
get_addr(input$LATITUDE[a:b],input$LONGITUDE[a:b])
# After each lookup (of 10^4 locations), the list is saved to disk.
cat("writing to file")
write.csv(out.query,
'./data/processing/OSM_ADDS.csv', row.names = F)
cat('Finished ',b, '/', maxn, 'lookups.',maxn-b, 'are left.')
}
b
length(get_addr(input$LATITUDE[a:b],input$LONGITUDE[a:b]))
length(out.query$addrss[input$idnum[a:b]])
input = read.csv('./data/output/Police_Interventions_cleaned.csv')
# The following function does the extraction of the full address given
# the coordinates.
get_addr = function(lat,long){
#pull=reverse_geo(lat = lat, long = long, method = 'osm', verbose = FALSE)
#addr = as.list(pull)$address
addr = rep("eh?",length(lat))
return(addr)
}
# We now apply the function to all the coordinates.
# Note that would take several hours to run.
# addrss: A list of all the full addresses.
out.query = data.frame(id=input$idnum, addrss=NA)
# subset the data so that we don't send NAs
input %>% filter(!is.na(LATITUDE)) -> input
# I will lookup 10^4 locations per request.
addrs.per.request = 1e4
maxn = nrow(input)
for(a in seq(1,maxn,addrs.per.request)){
b  = min(a+addrs.per.request-1,maxn)
out.query$addrss[input$idnum[a:b]] =
get_addr(input$LATITUDE[a:b],input$LONGITUDE[a:b])
# After each lookup (of 10^4 locations), the list is saved to disk.
cat("writing to file\n")
write.csv(out.query,
'./data/processing/OSM_ADDS.csv', row.names = F)
cat('Finished ',b, '/', maxn, 'lookups.',maxn-b, 'are left.')
}
nrow(input)
b
input$idnum[a:b] %>% view()
input$idnum[a:b] %>% max
outquery[191932]
out.query[191932]
require(lubridate)
require(tidyverse)
require(openxlsx)
"
June 2021
Police Interventions from April 1, 2015 to May 30, 2021 to be cleaned in this file.
Input files in the following directories are expected:
./data/input/interventionscitoyendo.csv
as downloaded from :
https://donnees.montreal.ca/ville-de-montreal/actes-criminels
This is the main data file containing the interventions.
./data/input/pdq.csv
as downloaded from :
https://donnees.montreal.ca/ville-de-montreal/carte-postes-quartier
This file contains extra informations on PDQ: Poste De Quartier:
./data/input/PDQ_BOR.xlsx
A hand prepared excel file containing the corresponding police divisions
(joint neighbourhoods) for each PDQ id number.
Credits goes to https://spvm.qc.ca
"
#-------------------------------------------------------------------
#-------- 1. Read data ----------------#
interv = read.csv('./data/input/interventionscitoyendo.csv')
pdq = read.csv('./data/input/pdq.csv')
spvm.pdq = read.xlsx('./data/input/PQD_BOR.xlsx',1,rowNames = F)
#-------------------------------------------------------------------
#-------- 2. Joining the first two datasets into one. --------#
#  The primary key in the three of them is the PDQ ID
# A string manipulation is required in one of them to extract the ID
pdq %>%
mutate(PDQ = mapply( function(d)
strsplit(d,'POSTE DE QUARTIER ')[[1]][2],DESC_LIEU),
LONGITUDE = NULL, LATITUDE = NULL) %>%
mutate(PDQ = as.numeric(PDQ)) -> pdq
data = left_join(interv, pdq, c('PDQ'))
#-------------------------------------------------------------------
#------- 3. Exploring & treating missing values. -------- #
# Whenever longitude or latitude are set to 1, they are missing
data %>%
mutate(LONGITUDE = replace(LONGITUDE, LONGITUDE==1, NA),
LATITUDE = replace(LATITUDE, LATITUDE==1, NA)) -> data
#--------------------
# analyzing NAs
print('Distribution of missing values')
colSums(is.na(data)) # shows #NA by column
print('Rows with missing pdq id')
data %>%
filter(is.na(PDQ))
' We Notice that we have 5 PDQ missing and 33125 coordiantions missing.
We will drop the 5 rows for now. As for the coordinations, we will later
use the pdq division (the third dataset) to estimate these missing values.
'
data %>% filter(!is.na(PDQ)) -> data
' Next we see how the missing coordinations are distributed
'
print("Missing Coordinations' distribution")
data %>%
filter(is.na(LONGITUDE)|is.na(LATITUDE))  %>%
select(CATEGORIE) %>%
table()
' Next we want to see how many of these missing coordinations have pdqs
That are not in the spvm$pdq dataset.
'
print("Missing Coordinations pdq distribution. -1: found in spvm.pdq")
data %>%
filter(is.na(LONGITUDE)|is.na(LATITUDE))  %>%
select(PDQ)  %>%
mutate(PDQ = ifelse(! PDQ %in% spvm.pdq$PDQ, PDQ, -1)) %>%
table()
' Result: We have only two unknown PDQs: 11 and 24.
From searching spvm.qc.ca we found that they have equivalent PDQs.
'
data$PDQ[data$PDQ==24] = 26
data$PDQ[data$PDQ==11] = 9
data$idnum = rownames(data)
#-------------------------------------------------------------------
#-------- 4. Merge the third dataset now that we matched the key column
data = left_join(data, spvm.pdq, c('PDQ')) #left_join doesn't sort the data
#-------------------------------------------------------------------
# --------- 5. Drop some of the unrelevant columns. -------
names(data)
data %>%
select(CATEGORIE, DATE, QUART, PDQ,
DIVISION, LONGITUDE, LATITUDE,idnum) -> data
#-------------------------------------------------------------------
#----------- 6. Translate some of the values from French to English
catg.fr = table(data$CATEGORIE) %>% names
catg.en = c("Resulting Death", "Break and Enter", "Mischief", "Auto Burglary",
"Auto theft", "Armed Robbery")
catg.rep = function(c) catg.en[which(catg.fr==c)]
quart.fr = table(data$QUART) %>% names
quart.en = c('Day', 'Night', 'Evening')
quart.rep = function(c) quart.en[which(quart.fr==c)]
# Apply changes and extract the Year and month
data %>% mutate(CATEGORIE = mapply(catg.rep,CATEGORIE),
DATE = as.Date(DATE, "%Y-%m-%d"),
QUART = mapply(quart.rep, QUART)) -> data
#-------------------------------------------------------------------
# --------- 7. Drop Some of the not-needed-anymore variables from the env.
rm(pdq, interv, spvm.pdq, catg.en, catg.fr, quart.en, quart.fr,
catg.rep, quart.rep)
#   -------- 8. Add two new columns for Month and Year
data$MONTH = month(data$DATE)
data$YEAR = year(data$DATE)
#-------------------------------------------------------------------
# -------- 8. Discard obsevations of the current month (June 2021)
data %>% filter( !(YEAR ==2021 & MONTH==6 )) -> data
#-----------------------------------------------------------------------
#------ 9. Save to disk or load from desk if it's been saved earlier
write.csv(data, './data/output/Police_Interventions_cleaned.csv',row.names = F)
#-------------------------------------
require(reticulate)
require(tidygeocoder)
require(lubridate)
require(tidyverse)
#========================================
input = read.csv('./data/output/Police_Interventions_cleaned.csv')
get_addr = function(lat,long){
#pull=reverse_geo(lat = lat, long = long, method = 'osm', verbose = FALSE)
#addr = as.list(pull)$address
addr = rep("eh?",length(lat))
return(addr)
}
out.query = data.frame(id=input$idnum, addrss=NA)
# subset the data so that we don't send NAs
input %>% filter(!is.na(LATITUDE)) -> input
# I will lookup 10^4 locations per request.
addrs.per.request = 1e4
maxn = nrow(input)
for(a in seq(1,maxn,addrs.per.request)){
b  = min(a+addrs.per.request-1,maxn)
out.query$addrss[input$idnum[a:b]] =
get_addr(input$LATITUDE[a:b],input$LONGITUDE[a:b])
# After each lookup (of 10^4 locations), the list is saved to disk.
cat("writing to file\n")
write.csv(out.query,
'./data/processing/OSM_ADDS.csv', row.names = F)
cat('Finished ',b, '/', maxn, 'lookups.',maxn-b, 'are left.')
}
require(lubridate)
require(tidyverse)
require(openxlsx)
"
June 2021
Police Interventions from April 1, 2015 to May 30, 2021 to be cleaned in this file.
Input files in the following directories are expected:
./data/input/interventionscitoyendo.csv
as downloaded from :
https://donnees.montreal.ca/ville-de-montreal/actes-criminels
This is the main data file containing the interventions.
./data/input/pdq.csv
as downloaded from :
https://donnees.montreal.ca/ville-de-montreal/carte-postes-quartier
This file contains extra informations on PDQ: Poste De Quartier:
./data/input/PDQ_BOR.xlsx
A hand prepared excel file containing the corresponding police divisions
(joint neighbourhoods) for each PDQ id number.
Credits goes to https://spvm.qc.ca
"
#-------------------------------------------------------------------
#-------- 1. Read data ----------------#
interv = read.csv('./data/input/interventionscitoyendo.csv')
pdq = read.csv('./data/input/pdq.csv')
spvm.pdq = read.xlsx('./data/input/PQD_BOR.xlsx',1,rowNames = F)
#-------------------------------------------------------------------
#-------- 2. Joining the first two datasets into one. --------#
#  The primary key in the three of them is the PDQ ID
# A string manipulation is required in one of them to extract the ID
pdq %>%
mutate(PDQ = mapply( function(d)
strsplit(d,'POSTE DE QUARTIER ')[[1]][2],DESC_LIEU),
LONGITUDE = NULL, LATITUDE = NULL) %>%
mutate(PDQ = as.numeric(PDQ)) -> pdq
data = left_join(interv, pdq, c('PDQ'))
#-------------------------------------------------------------------
#------- 3. Exploring & treating missing values. -------- #
# Whenever longitude or latitude are set to 1, they are missing
data %>%
mutate(LONGITUDE = replace(LONGITUDE, LONGITUDE==1, NA),
LATITUDE = replace(LATITUDE, LATITUDE==1, NA)) -> data
#--------------------
# analyzing NAs
print('Distribution of missing values')
colSums(is.na(data)) # shows #NA by column
print('Rows with missing pdq id')
data %>%
filter(is.na(PDQ))
' We Notice that we have 5 PDQ missing and 33125 coordiantions missing.
We will drop the 5 rows for now. As for the coordinations, we will later
use the pdq division (the third dataset) to estimate these missing values.
'
data %>% filter(!is.na(PDQ)) -> data
' Next we see how the missing coordinations are distributed
'
print("Missing Coordinations' distribution")
data %>%
filter(is.na(LONGITUDE)|is.na(LATITUDE))  %>%
select(CATEGORIE) %>%
table()
' Next we want to see how many of these missing coordinations have pdqs
That are not in the spvm$pdq dataset.
'
print("Missing Coordinations pdq distribution. -1: found in spvm.pdq")
data %>%
filter(is.na(LONGITUDE)|is.na(LATITUDE))  %>%
select(PDQ)  %>%
mutate(PDQ = ifelse(! PDQ %in% spvm.pdq$PDQ, PDQ, -1)) %>%
table()
' Result: We have only two unknown PDQs: 11 and 24.
From searching spvm.qc.ca we found that they have equivalent PDQs.
'
data$PDQ[data$PDQ==24] = 26
data$PDQ[data$PDQ==11] = 9
#-------------------------------------------------------------------
#-------- 4. Merge the third dataset now that we matched the key column
data = left_join(data, spvm.pdq, c('PDQ')) #left_join doesn't sort the data
#-------------------------------------------------------------------
# --------- 5. Drop some of the unrelevant columns. -------
names(data)
data %>%
select(CATEGORIE, DATE, QUART, PDQ,
DIVISION, LONGITUDE, LATITUDE,idnum) -> data
#-------------------------------------------------------------------
#----------- 6. Translate some of the values from French to English
catg.fr = table(data$CATEGORIE) %>% names
catg.en = c("Resulting Death", "Break and Enter", "Mischief", "Auto Burglary",
"Auto theft", "Armed Robbery")
catg.rep = function(c) catg.en[which(catg.fr==c)]
quart.fr = table(data$QUART) %>% names
quart.en = c('Day', 'Night', 'Evening')
quart.rep = function(c) quart.en[which(quart.fr==c)]
# Apply changes and extract the Year and month
data %>% mutate(CATEGORIE = mapply(catg.rep,CATEGORIE),
DATE = as.Date(DATE, "%Y-%m-%d"),
QUART = mapply(quart.rep, QUART)) -> data
#-------------------------------------------------------------------
# --------- 7. Drop Some of the not-needed-anymore variables from the env.
rm(pdq, interv, spvm.pdq, catg.en, catg.fr, quart.en, quart.fr,
catg.rep, quart.rep)
#   -------- 8. Add two new columns for Month and Year
data$MONTH = month(data$DATE)
data$YEAR = year(data$DATE)
#-------------------------------------------------------------------
# -------- 8. Discard obsevations of the current month (June 2021)
data %>% filter( !(YEAR ==2021 & MONTH==6 )) -> data
data$idnum = rownames(data)
#-----------------------------------------------------------------------
#------ 9. Save to disk or load from desk if it's been saved earlier
write.csv(data, './data/output/Police_Interventions_cleaned.csv',row.names = F)
#-------------------------------------
require(reticulate)
require(tidygeocoder)
require(lubridate)
require(tidyverse)
#========================================
input = read.csv('./data/output/Police_Interventions_cleaned.csv')
# The following function does the extraction of the full address given
# the coordinates.
get_addr = function(lat,long){
#pull=reverse_geo(lat = lat, long = long, method = 'osm', verbose = FALSE)
#addr = as.list(pull)$address
addr = rep("eh?",length(lat))
return(addr)
}
# We now apply the function to all the coordinates.
# Note that would take several hours to run.
# addrss: A list of all the full addresses.
out.query = data.frame(id=input$idnum, addrss=NA)
# subset the data so that we don't send NAs
input %>% filter(!is.na(LATITUDE)) -> input
# I will lookup 10^4 locations per request.
addrs.per.request = 1e4
maxn = nrow(input)
for(a in seq(1,maxn,addrs.per.request)){
b  = min(a+addrs.per.request-1,maxn)
out.query$addrss[input$idnum[a:b]] =
get_addr(input$LATITUDE[a:b],input$LONGITUDE[a:b])
# After each lookup (of 10^4 locations), the list is saved to disk.
cat("writing to file\n")
write.csv(out.query,
'./data/processing/OSM_ADDS.csv', row.names = F)
cat('Finished ',b, '/', maxn, 'lookups.',maxn-b, 'are left.')
}
View(out.query)
